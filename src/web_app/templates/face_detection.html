Spotify Music Recommender Based on Human Emotion
A machine learning-powered web application that recommends Spotify music based on detected human emotions through facial expressions and text analysis.

ğŸ¯ Project Overview
This final year project combines computer vision, natural language processing, and music information retrieval to create an intelligent music recommendation system. The application analyzes user emotions through two methods:

Facial Emotion Detection: Using computer vision to analyze facial expressions
Text Emotion Analysis: Using NLP to understand emotional content in text
Based on the detected emotions, the system recommends suitable music tracks from Spotify's vast library.

ğŸš€ Features
Real-time Facial Emotion Detection: Upload images or use webcam for emotion analysis
Text Emotion Analysis: Analyze emotional content from user text input
Smart Music Recommendations: AI-powered recommendations based on detected emotions
Spotify Integration: Direct access to Spotify's music library and features
Interactive Web Interface: Modern, responsive web application
Emotion-Music Mapping: Intelligent mapping between emotions and musical features
Track Similarity: Find similar tracks based on audio features
ğŸ› ï¸ Technology Stack
Backend
Python 3.8+
Flask: Web framework
TensorFlow/Keras: Deep learning for emotion detection
OpenCV: Computer vision operations
scikit-learn: Machine learning algorithms
Spotipy: Spotify Web API wrapper
Frontend
HTML5/CSS3/JavaScript
Bootstrap 5: Responsive design
Font Awesome: Icons
APIs & Libraries
Spotify Web API: Music data and features
FER: Facial emotion recognition
TextBlob & VADER: Text sentiment analysis
Pandas/NumPy: Data manipulation
ğŸ“ Project Structure
spotify-emotion-recommender/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ emotion_detection/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ face_emotion.py      # Facial emotion detection
â”‚   â”‚   â””â”€â”€ text_emotion.py      # Text emotion analysis
â”‚   â”œâ”€â”€ music_analysis/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ spotify_client.py    # Spotify API integration
â”‚   â”‚   â””â”€â”€ music_features.py    # Music feature analysis
â”‚   â”œâ”€â”€ recommendation/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ recommender.py       # Recommendation engine
â”‚   â””â”€â”€ web_app/
â”‚       â”œâ”€â”€ app.py               # Flask application
â”‚       â”œâ”€â”€ static/              # CSS, JS files
â”‚       â””â”€â”€ templates/           # HTML templates
â”œâ”€â”€ data/                        # Data storage
â”œâ”€â”€ models/                      # Trained models
â”œâ”€â”€ uploads/                     # Temporary file uploads
â”œâ”€â”€ requirements.txt             # Python dependencies
â”œâ”€â”€ .env                         # Environment variables
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md
âš™ï¸ Installation & Setup
Prerequisites
Python 3.8 or higher
Spotify Developer Account
Webcam (optional, for real-time detection)
Step 1: Clone the Repository
bash
git clone https://github.com/yourusername/spotify-emotion-recommender.git
cd spotify-emotion-recommender
Step 2: Create Virtual Environment
bash
python -m venv venv

# On Windows
venv\Scripts\activate

# On macOS/Linux
source venv/bin/activate
Step 3: Install Dependencies
bash
pip install -r requirements.txt
Step 4: Spotify API Setup
Go to Spotify Developer Dashboard
Create a new application
Get your Client ID and Client Secret
Create a .env file in the project root:
env
SPOTIFY_CLIENT_ID=your_spotify_client_id_here
SPOTIFY_CLIENT_SECRET=your_spotify_client_secret_here
FLASK_SECRET_KEY=your-secret-key-for-flask-sessions
Step 5: Run the Application
bash
python src/web_app/app.py
The application will be available at http://localhost:5000

ğŸµ How It Works
Emotion Detection Process
Facial Emotion Detection:
Uses FER (Facial Emotion Recognition) library
Detects 7 basic emotions: happy, sad, angry, fear, surprise, disgust, neutral
Analyzes facial landmarks and expressions
Returns confidence scores for each emotion
Text Emotion Analysis:
Combines VADER sentiment analysis and TextBlob
Uses keyword-based emotion detection
Analyzes sentiment polarity and subjectivity
Maps text emotions to musical preferences
Music Recommendation Process
Feature Extraction:
Extracts audio features from Spotify tracks
Features include: valence, energy, danceability, tempo, acousticness, etc.
Builds a comprehensive music database
Emotion-Music Mapping:
Maps detected emotions to musical characteristics
Happy â†’ High valence, high energy, danceable
Sad â†’ Low valence, low energy, acoustic
Angry â†’ Low valence, high energy, intense
Calm â†’ Medium valence, low energy, peaceful
Recommendation Algorithm:
Uses cosine similarity for track matching
Applies K-means clustering for music categorization
Considers multiple factors: emotion, user preferences, track popularity
ğŸ“Š Emotion-Music Mapping
Emotion	Valence	Energy	Danceability	Tempo	Musical Style
Happy	0.8	0.7	0.8	120+ BPM	Upbeat, Pop
Sad	0.2	0.3	0.3	60-90 BPM	Ballads, Acoustic
Angry	0.1	0.9	0.6	140+ BPM	Rock, Metal
Calm	0.5	0.3	0.4	70-100 BPM	Ambient, Chill
Fear	0.2	0.4	0.2	80-110 BPM	Dark, Atmospheric
ğŸ”§ API Endpoints
POST /api/analyze-text - Analyze text emotion
POST /api/analyze-image - Analyze facial emotion from image
GET /api/search-tracks - Search Spotify tracks
GET /api/similar-tracks/<track_id> - Get similar tracks
POST /api/recommend-by-features - Custom recommendations
GET /api/emotion-stats - Emotion distribution statistics
ğŸ¨ User Interface
Main Features:
Home Page: Overview and navigation
Face Detection: Upload images or use webcam
Text Analysis: Input text for emotion analysis
Results Display: Show detected emotions and recommendations
Music Player: Preview recommended tracks
Design Elements:
Spotify-inspired color scheme (green and black)
Responsive design for all devices
Interactive cards and animations
Real-time emotion feedback
Music visualization components
ğŸ“ˆ Model Performance
Facial Emotion Detection:
Accuracy: ~85-90% on standard datasets
Real-time Processing: 15-30 FPS depending on hardware
Supported Emotions: 7 basic emotions
Text Emotion Analysis:
Accuracy: ~80-85% on text classification
Processing Speed: Near real-time
Language Support: English (primary)
Music Recommendations:
Relevance Score: Based on cosine similarity
Database Size: 10,000+ tracks across multiple genres
Update Frequency: Real-time via Spotify API
ğŸš€ Deployment
Local Development:
bash
python src/web_app/app.py
Production Deployment:
Heroku:
bash
   # Create Procfile
   echo "web: gunicorn src.web_app.app:app" > Procfile
   
   # Deploy
   heroku create your-app-name
   git push heroku main
Docker:
dockerfile
   FROM python:3.9
   COPY requirements.txt .
   RUN pip install -r requirements.txt
   COPY . .
   CMD ["python", "src/web_app/app.py"]
ğŸ§ª Testing
Run Tests:
bash
# Unit tests
python -m pytest tests/

# Integration tests
python -m pytest tests/integration/

# API tests
python -m pytest tests/api/
Test Coverage:
Emotion detection accuracy tests
API endpoint tests
Database integration tests
Frontend functionality tests
ğŸ“ Future Enhancements
Planned Features:
Multi-language Support: Text analysis in multiple languages
Voice Emotion Detection: Analyze emotions from speech
User Profiles: Personalized recommendations based on history
Playlist Generation: Create custom playlists based on mood
Social Features: Share mood-based playlists
Mobile App: React Native or Flutter mobile application
Real-time Mood Tracking: Continuous emotion monitoring
Advanced ML Models: Deep learning for better accuracy
Technical Improvements:
Caching System: Redis for faster recommendations
Database Integration: PostgreSQL for user data
API Rate Limiting: Better handling of Spotify API limits
Monitoring: Application performance monitoring
CI/CD Pipeline: Automated testing and deployment
ğŸ¤ Contributing
Fork the repository
Create a feature branch: git checkout -b feature-name
Commit changes: git commit -am 'Add new feature'
Push to branch: git push origin feature-name
Submit a Pull Request
Development Guidelines:
Follow PEP 8 style guidelines
Write comprehensive tests
Update documentation for new features
Use meaningful commit messages
ğŸ“„ License
This project is licensed under the MIT License - see the LICENSE file for details.

ğŸ™ Acknowledgments
Spotify Web API: For providing access to music data
FER Library: For facial emotion recognition
OpenCV Community: For computer vision tools
Flask Community: For the web framework
Academic Supervisors: For guidance and support
ğŸ“ Contact
Student Name: Your Name
Email: your.email@university.edu
GitHub: @yourusername
LinkedIn: Your LinkedIn

University: Your University Name
Department: Computer Science/Engineering
Project Year: 2024

ğŸ› Issues and Bug Reports
Please report issues on the GitHub Issues page.

ğŸ“š References
Ekman, P. (1992). "An argument for basic emotions"
Spotify Web API Documentation
"Music Emotion Recognition: A State of the Art Review" - IEEE
"Facial Expression Recognition using Deep Learning" - Recent Papers
"Content-Based Music Recommendation Systems" - ACM Digital Library
This project is a final year academic project demonstrating the integration of AI, machine learning, and music technology for creating intelligent user experiences.

